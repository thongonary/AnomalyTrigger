{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mgpu-4-culture-plate-sm\u001b[0m  Fri Jul  5 09:18:34 2019\n",
      "\u001b[0;36m[0]\u001b[0m \u001b[0;34mGeForce GTX 1080\u001b[0m |\u001b[1;31m 77'C\u001b[0m, \u001b[1;32m 93 %\u001b[0m | \u001b[0;36m\u001b[1;33m 1767\u001b[0m / \u001b[0;33m 8114\u001b[0m MB | \u001b[1;30memoreno\u001b[0m(\u001b[0;33m1757M\u001b[0m)\n",
      "\u001b[0;36m[1]\u001b[0m \u001b[0;34mGeForce GTX 1080\u001b[0m |\u001b[0;31m 27'C\u001b[0m, \u001b[0;32m  0 %\u001b[0m | \u001b[0;36m\u001b[1;33m    0\u001b[0m / \u001b[0;33m 8114\u001b[0m MB |\n",
      "\u001b[0;36m[2]\u001b[0m \u001b[0;34mGeForce GTX 1080\u001b[0m |\u001b[0;31m 34'C\u001b[0m, \u001b[0;32m  0 %\u001b[0m | \u001b[0;36m\u001b[1;33m    0\u001b[0m / \u001b[0;33m 8114\u001b[0m MB |\n",
      "\u001b[0;36m[3]\u001b[0m \u001b[0;34mGeForce GTX 1080\u001b[0m |\u001b[0;31m 24'C\u001b[0m, \u001b[0;32m  0 %\u001b[0m | \u001b[0;36m\u001b[1;33m    0\u001b[0m / \u001b[0;33m 8114\u001b[0m MB |\n",
      "\u001b[0;36m[4]\u001b[0m \u001b[0;34mGeForce GTX 1080\u001b[0m |\u001b[0;31m 26'C\u001b[0m, \u001b[0;32m  0 %\u001b[0m | \u001b[0;36m\u001b[1;33m    0\u001b[0m / \u001b[0;33m 8114\u001b[0m MB |\n",
      "\u001b[0;36m[5]\u001b[0m \u001b[0;34mGeForce GTX 1080\u001b[0m |\u001b[0;31m 33'C\u001b[0m, \u001b[0;32m  0 %\u001b[0m | \u001b[0;36m\u001b[1;33m    0\u001b[0m / \u001b[0;33m 8114\u001b[0m MB |\n",
      "\u001b[0;36m[6]\u001b[0m \u001b[0;34mGeForce GTX 1080\u001b[0m |\u001b[0;31m 28'C\u001b[0m, \u001b[0;32m  0 %\u001b[0m | \u001b[0;36m\u001b[1;33m 5791\u001b[0m / \u001b[0;33m 8114\u001b[0m MB | \u001b[1;30mthong\u001b[0m(\u001b[0;33m5781M\u001b[0m)\n",
      "\u001b[0;36m[7]\u001b[0m \u001b[0;34mGeForce GTX 1080\u001b[0m |\u001b[1;31m 70'C\u001b[0m, \u001b[1;32m 92 %\u001b[0m | \u001b[0;36m\u001b[1;33m 5797\u001b[0m / \u001b[0;33m 8114\u001b[0m MB | \u001b[1;30mthong\u001b[0m(\u001b[0;33m5787M\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from glob import glob\n",
    "import sys, scipy\n",
    "from scipy.stats import chi2\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import gpustat\n",
    "gpustat.print_gpustat()\n",
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\" # Some versions of HDF5 require this\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2' # This is to choose which GPU to use\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader # \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LENGTH = 20 # number of particles to consider in 1 event\n",
    "INPUT_FEATURE = 3 # number of features. Only use pt, eta, phi now\n",
    "BATCH_SIZE = 10\n",
    "PT_SCALE = 10\n",
    "MAX_EPOCH = 20\n",
    "# Maurizio's Delphes data\n",
    "# this is the location on the caltech machine. Copied from /eos/project/d/dshep/TOPCLASS/L1jetLepData/\n",
    "base_dir = '/bigdata/shared/L1AnomalyDetection/qcd_lepFilter_13TeV/' \n",
    "bsm_dir = '/bigdata/shared/L1AnomalyDetection/Ato4l_lepFilter_13TeV/'\n",
    "# particle is ordered: MET + 10 e + 10 mu + 20 jet\n",
    "# We will take 4 ele + 4 muon + 12 jets = 20 objects in total\n",
    "\n",
    "PARTICLE_TO_USE = np.asarray([False]*41)\n",
    "PARTICLE_TO_USE[1:5] = True # 4 ele\n",
    "PARTICLE_TO_USE[11:15] = True # 4 muon\n",
    "PARTICLE_TO_USE[21:33] = True # 12 jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimpleEventSequence(Dataset):\n",
    "#     def __init__(self, data_x, data_y):\n",
    "#         self.len = data_x.shape[0]\n",
    "#         self.data_x = torch.from_numpy(data_x).float()\n",
    "#         self.data_y = torch.from_numpy(data_y)\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return self.len\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return (self.data_x[idx], self.data_y[idx])\n",
    "\n",
    "class EventSequence(Dataset):\n",
    "    def check_data(self, file_names):\n",
    "        num_data = 0\n",
    "        thresholds = [0]\n",
    "        for in_file_name in file_names:\n",
    "            h5_file = h5py.File( in_file_name, 'r' )\n",
    "            X = h5_file[self.feature_name]\n",
    "            if hasattr(X, 'keys'):\n",
    "                num_data += len(X[X.keys()[0]])\n",
    "                thresholds.append(num_data)\n",
    "            else:\n",
    "                num_data += len(X)\n",
    "                thresholds.append(num_data)\n",
    "            h5_file.close()\n",
    "        return (num_data, thresholds)\n",
    "\n",
    "    def __init__(self, dir_name, feature_name = 'Particles', sequence_length=50, verbose=False):\n",
    "        self.feature_name = feature_name\n",
    "        self.file_names = glob(dir_name+'/*.h5')\n",
    "        self.num_data, self.thresholds = self.check_data(self.file_names)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.file_index = 0\n",
    "        self.h5_file = h5py.File(self.file_names[self.file_index],'r')\n",
    "        self.get_data()\n",
    "        self.verbose=verbose\n",
    "        \n",
    "    def get_data(self):\n",
    "        self.X = np.array(self.h5_file.get(self.feature_name))[:,PARTICLE_TO_USE,:] # Skip the class feature \n",
    "\n",
    "    def is_numpy_array(self, data):\n",
    "        return isinstance(data, np.ndarray)\n",
    "\n",
    "    def get_num_samples(self, data):\n",
    "        \"\"\"Input: dataset consisting of a numpy array or list of numpy arrays.\n",
    "            Output: number of samples in the dataset\"\"\"\n",
    "        if self.is_numpy_array(data):\n",
    "            return len(data)\n",
    "        else:\n",
    "            return len(data[0])\n",
    "\n",
    "    def get_index(self, idx):\n",
    "        \"\"\"Translate the global index (idx) into local indexes,\n",
    "        including file index and event index of that file\"\"\"\n",
    "        file_index = next(i for i,v in enumerate(self.thresholds) if v > idx)\n",
    "        file_index -= 1\n",
    "        event_index = idx - self.thresholds[file_index]\n",
    "        return file_index, event_index\n",
    "\n",
    "    def get_thresholds(self):\n",
    "        return self.thresholds\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_index, event_index = self.get_index(idx)\n",
    "        \n",
    "        if file_index != self.file_index:\n",
    "            self.h5_file.close()\n",
    "            self.file_index = file_index\n",
    "            if self.verbose: \n",
    "                print(\"Opening new file {}\".format(self.file_names[self.file_index]))\n",
    "            self.h5_file = h5py.File(self.file_names[self.file_index],'r')\n",
    "            self.get_data()\n",
    "            self.X = shuffle(self.X)\n",
    "        #return [self.X[event_index], np.argmax(self.Y[event_index])]\n",
    "        X_ = self.X[event_index]\n",
    "        if not np.any(X_): # empty array, go to the next event\n",
    "            #print(\"Empty event. Skipping.\")\n",
    "            return self.__getitem__(idx+1)\n",
    "        #print(\"Getting event.\")\n",
    "        # Scale down pT\n",
    "        X = X_\n",
    "        X[:,0] = X_[:,0]/PT_SCALE\n",
    "        \n",
    "        #X = X.view((INPUT_LENGTH*INPUT_FEATURE,) # flatten\n",
    "        X = X[:,:3].flatten()\n",
    "        return X\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_features, int(n_features*2/3)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(int(n_features*2/3), int(n_features/3)),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(int(n_features/3), int(n_features*2/3)),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(int(n_features*2/3), n_features)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a bit long, because it needs to open through all \n",
    "# files to get the number of events in each files\n",
    "train_loader = DataLoader(EventSequence(dir_name=base_dir,\n",
    "                                    feature_name ='Particles', sequence_length=INPUT_LENGTH, verbose=False), \n",
    "                                    batch_size = BATCH_SIZE, shuffle=False,num_workers=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=60, out_features=40, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Linear(in_features=40, out_features=20, bias=True)\n",
      "    (3): ReLU(inplace)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=40, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Linear(in_features=40, out_features=60, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Trainable parameters: 6560\n"
     ]
    }
   ],
   "source": [
    "FLAT_FEATURES = INPUT_LENGTH*INPUT_FEATURE\n",
    "model = Autoencoder(FLAT_FEATURES).cuda()\n",
    "print(model)\n",
    "trainablePars = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('\\nTrainable parameters:', trainablePars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], loss:0.0493\n",
      "Epoch [2/20], loss:0.0504\n",
      "Epoch [3/20], loss:0.0503\n",
      "Epoch [4/20], loss:0.0507\n",
      "Epoch [5/20], loss:0.0509\n",
      "Epoch [6/20], loss:0.0499\n",
      "Epoch [7/20], loss:0.0505\n",
      "Epoch [8/20], loss:0.0499\n",
      "Epoch [9/20], loss:0.0496\n",
      "Epoch [10/20], loss:0.0483\n",
      "Epoch [11/20], loss:0.0485\n",
      "Epoch [12/20], loss:0.0484\n",
      "Epoch [13/20], loss:0.0482\n",
      "Epoch [14/20], loss:0.0481\n",
      "Epoch [15/20], loss:0.0491\n",
      "Epoch [16/20], loss:0.0493\n",
      "Epoch [17/20], loss:0.0492\n",
      "Epoch [18/20], loss:0.0493\n",
      "Epoch [19/20], loss:0.0508\n",
      "Epoch [20/20], loss:0.0489\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, \n",
    "                              mode='min',\n",
    "                              factor=0.3,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              threshold=1e-4,\n",
    "                              cooldown=2,\n",
    "                              min_lr=1e-7\n",
    "                             )\n",
    "\n",
    "train_loss = []\n",
    "loss_history = {'train': [], 'val': []}\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    batch_loss = []\n",
    "    for batch_idx, local_x in enumerate(train_loader):\n",
    "        local_x = Variable(local_x).float().cuda() \n",
    "        # I use an old version of Pytorch. For version > 0.4, feel free to remove Variable wrapper\n",
    "        x_prime = model(local_x)\n",
    "        loss = criterion(x_prime, local_x)\n",
    "        batch_loss.append(loss.data[0])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, MAX_EPOCH, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
